	一、Hive的存储格式及压缩
1.Hive使用的文件格式
  : SEQUENCEFILE//序列化		#二进制格式，无法使用cat查看
  | TEXTFILE    -- (Default, depending on hive.default.fileformat configuration)
  | RCFILE      -- (Note: Available in Hive 0.6.0 and later)  Row存储  Columnar字段也就是列
  | ORC         -- (Note: Available in Hive 0.11.0 and later) O optimized 优化
  | PARQUET     -- (Note: Available in Hive 0.13.0 and later)
  | AVRO        -- (Note: Available in Hive 0.14.0 and later)

可以将默认TEXTFILE 文件格式的表映射完成之后，另外保存成其他格式ORC PARQUET

create table if not exists file_text(
t_time string,
t_url string,
t_uuid string,
t_refered_url string,
t_ip string,
t_user string,
t_city string
)
row format delimited fields terminated by '\t'
stored  as  textfile;

load data local inpath '/home/user01/page_views.data' into table file_text;

//默认的TEXTFILE格式大小
dfs -du -s -h /user/hive/warehouse/db01.db/file_text;
18.1 M  18.1 M  /user/hive/warehouse/db01.db/file_text

//存储为ORC格式
create table if not exists  file_orc row format delimited fields terminated by '\t' stored as ORC as select *  from file_text;

dfs -du -s -h /user/hive/warehouse/db01.db/file_orc;
2.6 M  2.6 M  /user/hive/warehouse/db01.db/file_orc

//存储为parquet格式
create table if not exists  file_parquet row format delimited fields terminated by '\t' stored as PARQUET as select *  from file_text;


 dfs -du -s -h  /user/hive/warehouse/db01.db/file_parquet;
13.1 M  13.1 M  /user/hive/warehouse/db01.db/file_parquet

【注意：】
	1.对于具体格式在创建表时一定要指定stored as orc/parquet/textfile
	2.插入数据的时候不能使用load加载数据

 2.压缩
减少磁盘存储压力，负载
	减少网络IO负载
1）.首先，要保证hadoop是支持压缩
检查是否支持压缩算法
		$ bin/hadoop checknative
		Native library checking:
hadoop:  false 
zlib:    false 
snappy:  false 
lz4:     false 
bzip2:   false 
openssl: false

snappy 压缩比和速度相当于适中的

2）编译hadoop源码：mvn package -Pdist,native,docs -DskipTests -Dtar  -Drequire.snappy

3）##替换$HADOOP_HOME/lib/native 直接上传到$HADOOP_HOME
$ tar -zxf cdh5.3.6-snappy-lib-natirve.tar.gz 
再次检查  $ bin/hadoop checknative
Native library checking:
hadoop:  true /opt/modules/cdh/hadoop-2.5.0-cdh5.3.6/lib/native/libhadoop.so.1.0.0
zlib:    true /lib64/libz.so.1
snappy:  true /opt/modules/cdh/hadoop-2.5.0-cdh5.3.6/lib/native/libsnappy.so.1
lz4:     true revision:99
bzip2:   true /lib64/libbz2.so.1
openssl: true /usr/lib64/libcrypto.so

启动dfs，yarn，historyserver，然后提交一个job
$ bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0-cdh5.3.6.jar pi 1 2
完成之后，web界面查看完成这个任务的配置

mapreduce.map.output.compress 	false 	
mapreduce.map.output.compress.codec 	org.apache.hadoop.io.compress.DefaultCodec 

4）配置mapred-site.xml，在原有的下方增加如下内容
	<property>
		<name>mapreduce.map.output.compress</name>
		<value>true</value>
	</property>
	
	<property>
		<name>mapreduce.map.output.compress.codec </name>
		<value>org.apache.hadoop.io.compress.SnappyCodec</value>
	</property>

结束所有进程，重新启动所有进程dfs，yarn，historyserver，再提交一个job
$ bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0-cdh5.3.6.jar pi 3 5
完成之后，web界面重新查看完成这个任务的配置


mapreduce.map.output.compress 	true 	job.xml ⬅ mapred-site.xml
mapreduce.map.output.compress.codec 	org.apache.hadoop.io.compress.SnappyCodec 


/启用压缩snappy+存储为ORC格式
方式一 在MapReduce的shuffle阶段启用压缩

set hive.exec.compress.output=true;
set mapred.output.compress=true;
set mapred.output.compression.codec=org apache.hadoop.io.compress.SnappyCodec;


create table if not exists file_orc_snappy(
t_time string,
t_url string,
t_uuid string,
t_refered_url string,
t_ip string,
t_user string,
t_city string
)
row format delimited fields terminated by '\t'
stored as  ORC
tblproperties("orc.compression"="Snappy");

insert into table file_orc_snappy select *  from file_text;

方式二：对reduce输出的结果文件进行压缩
set mapreduce.output.fileoutputformat.compress=true;
set mapreduce.output.fileoutputformat.compress.codec=org apache.hadoop.io.compress.SnappyCodec;

create table if not exists file_parquet_snappy(
t_time string,
t_url string,
t_uuid string,
t_refered_url string,
t_ip string,
t_user string,
t_city string
)
row format delimited fields terminated by '\t'
stored as parquet
tblproperties("parquet.compression"="Snappy");

insert into table file_parquet_snappy select * from file_text;
insert overwrite table file_parquet_snappy select * from file_text;

hive (mydb)> dfs -du -s -h /user/hive/warehouse/mydb.db/file_parquet_snappy;
6.4 M  6.4 M  /user/hive/warehouse/mydb.db/file_parquet_snappy
hive (mydb)> dfs -du -s -h /user/hive/warehouse/mydb.db/file_parquet;       
13.1 M  13.1 M  /user/hive/warehouse/mydb.db/file_parquet

二、【案例二】
【案例二】通过正则匹配，映射复杂格式日志文件为Hive表格
"27.38.5.159" "-" "31/Aug/2015:00:04:37 +0800" "GET /course/view.php?id=27 HTTP/1.1" "303" "440" - "http://www.ibeifeng.com/user.php?act=mycourse" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36" "-" "learn.ibeifeng.com"

(\"[^ ]\") (\"-[^ ]\")
^ 一个字符串的开始
$ 一个字符串的结束
"^The"  "There"  "The cat"
e$ 以字符e结尾的字符串

一个或者一序列字符重复出现的次数
* 0次或多次 {0,}
+ 1次或多次 {1,}
? 至多出现一次  0次或者1次  {0,1}

{}使用范围，用大括号，表示重复出现次数的范围
“ab{2}” 表示一个字符串有一个a跟着2个b
ab{2，4} 

[] 表示某些字符允许在一个字符串中某一个特定位置出现  
^[a-zA-Z]:标表示一个以字母开头的字符串

|  表示“或”操作
hi|hello   一个字符串里有"hi"或者"hello"
 (b|cd)ef   表示"bef"或"cdef"

[^] 表示不希望出现的字符
$ () "" 希望匹配这些字符的时候，应该在特殊字符前面加转义字符"\"

(\"[^ ]*\") (\"-|[^ ]*\") (\"[^\]]*\") (\"[^-%]*\") (\"[0-9]*\") (\"[0-9]*\") (-|[^ ]*) (\"[^ ]*\") (\"[^\"]*\") (\"-|[^ ]*\") (\"[^ ]*\")

 "27.38.5.159" "-" "31/Aug/2015:00:04:37 +0800" "GET /course/view.php?id=27 HTTP/1.1" "303" "440" - "http://www.ibeifeng.com/user.php?act=mycourse" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36" "-" "learn.ibeifeng.com"

drop table if exists apachelog;

 CREATE TABLE apachelog (
remote_addr string,
remote_user string,
time_local string,
request string,
status string,
body_bytes_set string,
request_body string,
http_referer string,
http_user_agent string,
http_x_forwarded_for string,
host string
 )
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
  "input.regex" = "(\"[^ ]*\") (\"-|[^ ]*\") (\"[^\]]*\") (\"[^\]]*\") (\"[0-9]*\") (\"[0-9]*\") (-|[^ ]*) (\"[^ ]*\") (\"[^\"]*\") (\"-|[^ ]*\") (\"[^ ]*\")"
)
STORED AS TEXTFILE;

创建临时函数的方式一：

//将导出的jar文件加入到hive的calss path
add jar /home/user01/RemoveQuote.jar;

//创建临时函数 removequote--可以自定义 但是必须指定对应的带包名的主类
CREATE TEMPORARY FUNCTION removequote as 'hive.RemoveQuoteUDF';

//查看创建的函数是否存在
show functions;

//将导出的jar文件加入到hive的calss path
add jar /home/user01/TimeFormat.jar;
Added /home/user01/TimeFormat.jar to class path
Added resource: /home/user01/TimeFormat.jar

//创建临时函数  注意：临时函数名称不能和内置函数名相同
CREATE TEMPORARY FUNCTION TimeFormat as 'hive.TimeFormatUDF';

创建临时函数的方式二：
在hive中使用（第二种方式）
	将linux中的jar包上传到自定义位置的hdfs的文件系统
	hive (mydb)> create [temporary] function my_lower as 'com.ibeifeng.bigdata.test.hive.udf.Lower' using jar 'hdfs/path';
	注：关于'hdfs/path' ：
		hdfs://hostname:8020/自定义的文件位置

三、Hive企业优化：
 1.	fetch task任务不走MapReduce，可以在hive配置文件中设置最大化(more)和最小化(minimal)fetch task任务；通常在使用hiveserver2时调整为more；

设置参数的优先级：在命令行或者代码设置参数 > hive-site.xml>hive-default.xml
set hive.fetch.task.conversion=more;   //单次交互模式下有效，

bin/hive --hiveconf hive.fetch.task.conversion=more

上面的两种方法都可以开启了Fetch任务，但是都是临时起作用的；如果你想一直启用这个功能，可以在${HIVE_HOME}/conf/hive-site.xml里面加入以下配置：
<property>
  <name>hive.fetch.task.conversion</name>
  <value>more</value>
  <description>
    Some select queries can be converted to single FETCH task 
    minimizing latency.Currently the query should be single 
    sourced not having any subquery and should not have
    any aggregations or distincts (which incurrs RS), 
    lateral views and joins.
    1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only
    2. more    : SELECT, FILTER, LIMIT only (+TABLESAMPLE, virtual columns)
  </description>
</property>

2.	strict mode：严格模式设置，严格模式下将会限制一些查询操作
	文件格式，ORC PARQUET 等   
	分区表
	select 查询不加where过滤条件，不会执行

开启严格模式
hive提供的严格模式，禁止3种情况下的查询模式。
a：当表为分区表时，where字句后没有分区字段和限制时，不允许执行。
b：当使用order by语句时，必须使用limit字段，因为order by 只会产生一个reduce任务。
c：限制笛卡尔积的查询。sql语句不加where不会执行
	
<property>
  <name>hive.mapred.mode</name>
  <value>nonstrict</value>
  <description>The mode in which the Hive operations are being performed.
     In strict mode, some risky queries are not allowed to run. They include:
       Cartesian Product.
       No partition being picked up for a query.
       Comparing bigints and strings.
       Comparing bigints and doubles.
       Orderby without limit.
  </description>
</property>


3.	优化sql语句，如先过滤再join，先分组再做distinct;
eg1.
Select count(*) cnt
From store_sales ss
     join household_demographics hd on (ss.ss_hdemo_sk = hd.hd_demo_sk)
     join time_dim t on (ss.ss_sold_time_sk = t.t_time_sk)
     join store s on (s.s_store_sk = ss.ss_store_sk)
Where
     t.t_hour = 8
     t.t_minute >= 30
     hd.hd_dep_count = 2
order by cnt;

eg2.
在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *。
在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤，比如：

SELECT a.empno
FROM emp a
left outer join emp_part b
ON (a.deptno = b.depno)
WHERE b.day = '20150828′;

         正确的写法是写在ON后面：

SELECT a.id
FROM emp a
left outer join emp_part b
ON (a.deptno = b.depno AND b.day = '2015082818');

或者直接写成子查询：

SELECT a.id
FROM emp a
left outer join (SELECT url FROM emp_part WHERE day = ‘2015082818′) b
ON (a.deptno = b.depno)

eg3.
少用COUNT DISTINCT
数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换：

SELECT day,
COUNT(DISTINCT id) AS uv
FROM emp
GROUP BY day

可以转换成：

SELECT day,
COUNT(id) AS uv
FROM (SELECT day,id FROM emp GROUP BY day,id) a
GROUP BY day;


4.	MapReduce过程的map、shuffle、reduce端的snappy压缩

	需要先替换hadoop的native本地包开启压缩
	在mapred-site.xml文件设置启用压缩及压缩编码
	在执行SQL执行时设置启用压缩和指定压缩编码

set mapreduce.output.fileoutputformat.compress=true;
set mapreduce.output.fileoutputformat.compress.codec=org apache.hadoop.io.compress.SnappyCodec;

5.	大表拆分成子表，提取中间结果集，减少每次加载数据	
	多维度分析，多个分析模块
	每个分析模块涉及字段不一样，而且并不是表的全部字段

6.	分区表及外部表
	设计二级分区表（一级字段为天，二级字段设置小时）
	创建的的是外部表，创建表时直接指定数据所在目录即可，不用再用load加载数据

7.设置map和reduce个数：默认情况下一个块对应一个map任务，map数据我们一般不去调整，reduce个数根据reduce处理的数据量大小进行适当调整
	体现“分而治之”的思想
	set mapred.reduce.tasks=5;

	split_size=max(min_split_size, min(max_split_size, block_size))
	CombineFileInputFormat

	确定map个数
	http://blog.csdn.net/dr_guo/article/details/51150278;

	合并小文件
	combinefileinputformat
	简单之美 | Hadoop MapReduce处理海量小文件：基于CombineFileInputFormat
http://shiyanjun.cn/archives/299.html

8.并行执行：一个sql有1、2、3 三个job，其中1、2job是在join后再与3job结果进行join，那么1、2job在join同时job3可以同时并行执行
	
	分布式并行化 
set hive.exec.parallel=true;
<description>Whether to execute jobs in parallel</description>

set hive.exec.parallel.thread.number=8;
<description>How many jobs at most can be executed in parallel</description>

eg:

select num 
from 
(select count(city) as num from city
union all
select count(province) as num from province
)tmp;	

select a.id,a.name
form  user a 
left join (select b.id,b.cityid from order) b on a,id=b.id 
join (select c.city,c.cityname from area)c on b.cityid=c.ctiyid;

9.JVM重用：一个job可能有多个map reduce任务，每个任务会开启一个JVM虚拟机，默认情况下一个任务对应一个JVM，任务运行完JVM即销毁，我们可以设置JVM重用参数，一般不超过5个，这样一个JVM内可以连续运行多个任务

JVM重用是Hadoop调优参数的内容，对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或者task特别多的场景，这类场景大多数执行时间都很短。hadoop默认配置是使用派生JVM来执行map和reduce任务的，这是jvm的启动过程可能会造成相当大的开销，尤其是执行的job包含有成千上万个task任务的情况。
JVM重用可以使得JVM实例在同一个JOB中重新使用N次，N的值可以在Hadoop的mapre-site.xml文件中进行设置（建议参考5~10）
mapred.job.reuse.jvm.num.tasks(旧版)
mapreduce.job.jvm.numtasks(新版)
hadoop.apache.org/docs/r2.5.2/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml
http://hadoop.apache.org/docs/r2.5.2/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml

也可在hive的执行设置：
***set mapred.job.reuse.jvm.num.tasks=10;
hive (default)> set mapred.job.reuse.jvm.num.tasks;
mapred.job.reuse.jvm.num.tasks=1


10.	推测执行：例如一个Job应用有10个MapReduce任务（map 及reduce），其中9个任务已经完成，那么application Master会在另外启动一个相同的任务来运行未完成的那个，最后哪个先运行完成就把另一个kill掉

	启用speculative最大的好处是，一个map执行的时候，系统会在其他空闲的服务器上启动相同的map来同时运行，哪个运行的快就使用哪个的结果，另一个运行慢的在有了结果之后就会被kill。

hive-site.xml
hive.mapred.reduce.tasks.speculative.execution=true;
<property>
  <name>hive.mapred.reduce.tasks.speculative.execution</name>
  <value>true</value>
  <description>Whether speculative execution for reducers should be turned on. </description>
</property>

11.查看执行计划
***explain [extended] hql
eg:
explain select no,count(*) from testudf group by no;
explain extended select no,count(*) from testudf group by no;



二、数据倾斜
对于普通的join操作，会在map端根据key的hash值，shuffle到某一个reduce上去，在reduce端做join连接操作，内存中缓存join左边的表，遍历右边的表，依次做join操作。所以在做join操作时候，将数据量多的表放在join的右边。

当数据量比较大，并且key分布不均匀，大量的key都shuffle到一个reduce上了，就出现了数据的倾斜。

常见的数据倾斜出现在group by和join..on..语句中。
join（数据倾斜）
在进行两个表join的过程中，由于hive都是从左向右执行，要注意讲小表在前，大表在后（小表会先进行缓存）。


map/reduce程序执行时，reduce节点大部分执行完毕，但是有一个或者几个reduce节点运行很慢，导致整个程序的处理时间很长，这是因为某一个key的条数比其他key多很多（有时是百倍或者千倍之多），这条key所在的reduce节点所处理的数据量比其他节点就大很多，从而导致某几个节点迟迟运行不完，此称之为数据倾斜。
hive在跑数据时经常会出现数据倾斜的情况，使的作业经常reduce完成在99%后一直卡住，最后的１%花了几个小时都没跑完，这种情况就很可能是数据倾斜的原因，

hive.groupby.skewindata=true; 
如果是group by过程出现倾斜应将此项设置true。
<property>
  <name>hive.groupby.skewindata</name>
  <value>false</value>
  <description>Whether there is skew in data to optimize group by queries</description>
</property>


hive.optimize.skewjoin.compiletime=true;
如果是join 过程中出现倾斜应将此项设置为true
不影响结果可以考虑过滤空值
<property>
  <name>hive.optimize.skewjoin.compiletime</name>
  <value>false</value>
</property>  

hive.optimize.skewjoin.compiletime=true; 如果是join过程出现倾斜应该设置为true 
此时会将join语句转化为两个mapreduce任务，第一个会给jion字段加随机散列
set hive.skewjoin.key=100000; 这个是join的键对应的记录条数超过这个值则会进行优化。

可以在空值前面加随机散列



4.1 Map-side Join
mapJoin的主要意思就是，当链接的两个表是一个比较小的表和一个特别大的表的时候，我们把比较小的table直接放到内存中去，然后再对比较大的表格进行map操作。join就发生在map操作的时候，每当扫描一个
大的table中的数据，就要去查看小表的数据，哪条与之相符，继而进行连接。这里的join并不会涉及reduce操作。map端join的优势就是在于没有shuffle，真好。在实际的应用中，我们这样设置：
***1.	set hive.auto.convert.join=true; 
这样设置，hive就会自动的识别比较小的表，继而用mapJoin来实现两个表的联合。看看下面的两个表格的连接。

<property>
  <name>hive.auto.convert.join.noconditionaltask.size</name>
  <value>10000000</value> The default is 10MB
 </property>

DistributedCache是分布式缓存的一种实现，它在整个MapReduce框架中起着相当重要的作用，他可以支撑我们写一些相当复杂高效的分布式程
 
这里的第一句话就是运行本地的map join任务，继而转存文件到XXX.hashtable下面，在给这个文件里面上传一个文件进行map join，之后才运行了MR代码去运行计数任务。说白了，在本质上mapjoin根本就没有运行MR进程，仅仅是在内存就进行了两个表的联合。
	
mapjoin使用场景
1.关联操作中有一张表非常小
2.不等值的链接操作

自动执行
set hive.auto.convert.join=true;
hive.auto.convert.join.noconditionaltask.size=25;推荐值是25mb   

<property>
  <name>hive.auto.convert.join.noconditionaltask.size</name>
  <value>25000000</value>
 </property>

手动执行 A为小表  如果A表超过25M，还想使用map join;
select /*+mapjoin(A)*/ f.a,f.b from A t join B f on(f.a==t.a)

hive入门学习：join的三种优化方式 - HAHA的专栏 - 博客频道 - CSDN.NET
http://blog.csdn.net/liyaohhh/article/details/50697519

4.2 Reduce-side Join
***hive join操作默认使用的就是reduce join
Reduce-side Join原理上要简单得多，它也不能保证相同key但分散在不同dataset中的数据能够进入同一个Mapper，整个数据集合的排序在Mapper之后的shuffle过程中完成。相对于Map-side Join，它不需要每个Mapper都去读取所有的dataset，这是好处，但也有坏处，即这样一来Mapper之后需要排序的数据集合会非常大，因此shuffle阶段的效率要低于Map-side Join。
***reduce side join是一种最简单的join方式，其主要思想如下：
在map阶段，map函数同时读取两个文件File1和File2，为了区分两种来源的key/value数据对，对每条数据打一个标签（tag）,比如：tag=0表示来自文件File1，tag=2表示来自文件File2。即：map阶段的主要任务是对不同文件中的数据打标签。
在reduce阶段，reduce函数获取key相同的来自File1和File2文件的value list， 然后对于同一个key，对File1和File2中的数据进行join（笛卡尔乘积）。即：reduce阶段进行实际的连接操作。

semi  join  小表对大表  是reudce join的变种 map阶段过滤掉不需要join的字段 相当于Hivw SQL加的where过滤 

***4.3 SMB Join（sort merge bucket）
SMB 存在的目的主要是为了解决大表与大表间的 Join 问题，分桶其实就是把大表化成了“小表”，然后 Map-Side Join 解决之，这是典型的分而治之的思想。
对于每一个表（table）或者分区， Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive也是 针对某一列进行桶的组织。Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。
smb是sort merge bucket操作，首先进行排序，继而合并，然后放到所对应的bucket中去，bucket是hive中和分区表类似的技术，就是按照key进行hash，相同的hash值都放到相同的bucket中去。再进行两个表联合的时候。我们首先进行分桶，在join会大幅度的对性能进行优化。也就是说，在进行联合的时候，是table1中的一小部分和table2中的一小部分进行联合，table联合都是等值连接，相同的key都放到了同一个bucket中去了，那么在联合的时候就会大幅度的减小无关项的扫描。
分桶：
set hive.enforce.bucketing=true;
set hive.enforce.sorting=true;
表优化数据目标：相同数据尽量聚集在一起

[小心得，小体会]
hive (db01)> set hive.exec.mode.local.auto=true;
hive (db01)> set hive.exec.mode.local.auto;       
hive.exec.mode.local.auto=true


bin/hive --hiveconf hive.fetch.task.conversion=more;
bin/hive --hiveconf hive.exec.mode.local.auto=true;


Hive远程模式
	生产环境下用
	存放元数据的MySQL数据库服务器和Hive服务器不在同一台上，甚至放在不同的操作系统上。

	【问题】
	如何确保元数据的安全？
		1.定期备份Mysql
		2.搭建高可用的MySql集群


Hive远程模式
	生产环境下用
	存放元数据的MySQL数据库服务器和Hive服务器不在同一台上，甚至放在不同的操作系统上。

	【问题】
	如何确保元数据的安全？
		1.定期备份Mysql
		2.搭建高可用的MySql集群
	metastore下面的表 引擎  MyIsam和Innode

	修改hive-site.xml
<property>
  <name>hive.metastore.uris</name>
  <value>thrift://[hostname]:9083</value>
  <description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>
</property>

开启metastore服务或者进程
$ bin/hive --service  metastore  &
$ bin/hive --service  hiveserver2  &     //beeline  jdbc

检测是否启动（查看端口号）
netstat -antp |  grep 9083
netstat -antp |  grep  100000


结束进程
使用上述查看端口对应的进程的命令找出pid，然后使用kill -9 pid
